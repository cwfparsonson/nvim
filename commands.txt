----------------------------------------------------
# TMUX
------------------------------------------------------
(<leader> == ctrl+,)
tmux source-file .tmux.conf (updated tmux to current .tmux.conf config)
tmux ls (check what is running)
tmux list-panes -a
tmux list-window
<leader>, d (close tmux window)

# Basic
# tmux (launch tmux)
<leader>, " (split panes horizontally)
<leader>, % (split panes vertically)
tmux move-pane -s 3:0
<leader> + dont release + arrow keys (resize pane)
<leader>, c (create new window)
<leader>, n (move to next window)
<leader>, p (move to previous window)
exit (close single tmux pane)
<leader>, :kill-session (close all windows and/or panes at once)
<leader> [ (go into copy mode, can page up/down terminal output)
q (quit copy mode)

tmux new -s <session-name> (create session)
tmux attach-session -t <session-name> (go into session already created)
tmux kill-session -t <session-name>

# Creating New SSH Tunnel (mammoth port: 6017, berlin port: 10017). https://docs.anaconda.com/anaconda/user-guide/tasks/remote-jupyter-notebook/
1. In terminal, ssh into final server you want to tunnel to
2. tmux new -s <name_of_session>
3. (tmux attach-session -t <session_name>) if didn't open automatically
3. conda activate <env_name> (env should have e.g. jupyter installed)
4. jupyter-notebook --port=mammoth_port_#### (port no. e.g. 6002). Make a note of the link with the token
5. ctrl+b, d (detach from tmux)
6. exit (go back to berlin server if remote accessing from laptop)
7. ssh -Nf -L berlin_port_####:localhost:mammoth_port_#### mammoth
8. exit (go back to local machine)
9. ssh -Nf -L mammoth_port_####:localhost:berlin_port_#### zciccwf@ssh.ee.ucl.ac.uk
10. localhost:mammoth_port_#### (type into internet search bar)
11. if prompted, copy-paste the token=<copy-paste-this-token> into prompt asking for password/token
# NOTE: If get error 'bind: Cannot assign requested address', is because is trying to bind on IPv6 address. Force IPv4 with -4 flag:
ssh -Nf -4 -L mammoth_port_####:localhost:berlin_port_#### zciccwf@ssh.ee.ucl.ac.uk
# N.B. In windows for step 9, do not do the -Nf flag, type into PowerShell, and do not close PowerShell. i.e. do:
ssh -L mammoth_port_####:localhost:berlin_port_#### zciccwf@ssh.ee.ucl.ac.uk

lsof -ti:<port_numer> (list all processes using port number)
lsof -ti:<port_number> | xargs kill -9 (kill all processes using port number)

----------------------------------------------------
# LINUX
------------------------------------------------------
ls -lah (list all files in dir in human readable format (size, date))
dpkg -L <packagename> (to find where package is installed)
pwd (print working directory)
grep -w <word> * (search all files in directory for word)
grep -r <word> * (recursively search all directories and subdirectories)
chown user <filename> (change ownership of file)
mv <oldfile.txt> <newfile.txt> (rename file) 
du -s -h <filename> (show the disk usage of a certain folder)
chmod -R 777 <filename> (give all users permissions to make directories under that directory)
dd if=/dev/zero of=/dev/sdX bs=1M #replace X with the target drive letter. (wipe a disk by filling it with zeros)
history (shows all previous commands used)
dmesg (prints our message buffer of kernel, good for debugging)
exit (exit ssh'd server)

grep -rnw -e '<str>' (recursively look for string in all child trees/dirs/files. -r=recursive,-n=line number,-w=match whole word,-l=file names of matches
grep -rl '<oldstr>' . | xargs sed -i 's/<oldstr>/<newstr>/g' (find and replace all oldstr with new str in all child trees/dirs/files)

# find and rename files in all sub directories from current directory (https://stackoverflow.com/questions/16541582/find-multiple-files-and-rename-them-in-linux)
find . -iname "<old_file_name>" -exec rename <old_file_name> <new_file_name> '{}' \;


watch nvidia-smi (check GPU usage status on servers)
top -i (list CPU usage on servers)
htop (monitor vital resource usage (RAM etc) on servers https://linuxtogether.org/htop-command-explanation/)

# transfer file from/to local machine to/from server (use scp -r to transfer folder)
# N.B. Must be on local machine when doing this!!
scp /home/cwfparsonson/Downloads/test.csv zciccwf@ssh.ee.ucl.ac.uk:/home/zciccwf/personal/projects/kaggle/titanic/files
scp zciccwf@ssh.ee.ucl.ac.uk:/home/zciccwf/phd_project/projects/deep_scheduler/code/src/dynamic_images.mp4 /home/cwfparsonson/Downloads

ps -ef | grep zciccwf # list running processes for user zciccwf (-ps -> list processes, -e -> show all processes, -f -> show processes in detailed full format, grep -> find lines containing a pattern)

showquota (show how much space you have left on server hard drive)
quota -u <user> (same as above but works if don't have certain packages installed)
# N.B. UCL server deleted files go to /home/zciccwf/.local/share/Trash/files
df -h (print disk free space in human readable format)
du -h (print disk utilisation in human readable format)
du -h . | sort -nr | head -n10 (list top 10 largest directories from current directory)
du -s * | sort -nr | head -n10 (list top 10 largest files from current directory)
du -h . | sort -rh (list largest files from current directory and sort in order of size)
du -h . | sort -rh | head -n30 (list top 30 folders and their respective memory usage as well as total usage of current dir)
du -cha --max-depth=1 / | grep -E "M|G" (list largest directorys in / (root) -> can then do e.g. /var/log to hone in on what is taking space)

# remove large log file safely
sudo bash -c 'cat /dev/null > /var/log/uvcdynctrl-udev.log'

# cap /var/log/journal to 100 MB and clear data beyond 10 days old (https://askubuntu.com/questions/1238214/big-var-log-journal)
journalctl --vacuum-size=100M
journalctl --vacuum-time=10d

# clear old headers and images (useful for freeing root memory)
dpkg -l 'linux-*' | sed '/^ii/!d;/'"$(uname -r | sed "s/\(.*\)-\([^0-9]\+\)/\1/")"'/d;s/^[^ ]* [^ ]* \([^ ]*\).*/\1/;/[0-9]/!d' | xargs sudo apt-get -y purge

# remove old kernels (useful for freeing memory)
sudo apt-get autoremove --purge

# list top 50 applications by memory usage
dpkg-query --show --showformat='${Package;-50}\t${Installed-Size}\n' | sort -k 2 -n | grep -v deinstall | awk '{printf "%.3f MB \t %s\n", $2/(1024), $1}'

# completely remove application
sudo apt purge <application_name>
>>>>>>> 3eec252e69a0ef8e96abe7dc223d72852d1a38a8

# list packages in estimated size order (in kB)
dpkg-query -Wf '${Installed-Size}\t${Package}\n' | sort -n
# this list includes packages that have been removed but not purged. To purge such packages, run:
dpkg --list |grep "^rc" | cut -d " " -f 3 | xargs sudo dpkg --purge
# remove softare application:
sudo apt-get remove <application_name>

# to be able to install things without sudo privileges on servers etc, go to https://docs.brew.sh/Homebrew-on-Linux and follow alternative installation instructions where git clone homebrew into /home directory. Can then brew install neovim etc
# when installed, run echo $PATH to see where your executable paths are, and move neovim installation to one of these paths and rename file to nvim to make nvim executable
# to get your config, git clone this nvim repo into ~/.config/

# download data from google drive (replace link w/ link where your data is)
gdown https://drive.google.com/uc?id=0B7EVK8r0v71pOXBhSUdJWU1MYUk

# unzip all gzip files in a directory
gunzip *.gz

# use tar and gzip to zip a directory https://unix.stackexchange.com/questions/93139/can-i-zip-an-entire-folder-using-gzip
tar -zcvf archive.tar.gz directory/ 

# ssh config location
/home/cwfparsonson/.ssh


------------------------------------------------------






----------------------------------------------------
# MYRIAD
------------------------------------------------------
https://www.rc.ucl.ac.uk/docs/Clusters/Myriad/
ssh zciccwf@myriad.rc.ucl.ac.uk

- has ~100 nodes (servers) each with 36 cores per node. These 100 nodes are made up of 3 different node types with different specs
- can use 1 core for up to 72 hours, or 2-36 cores for up to 48 hours
- check memory quota with lquota. Get 150 GB for /home/zciccwf/, and 1 TB for /scratch/scratch/zciccwf/













------------------------------------------------------
# NEOVIM
------------------------------------------------------
# Basic
h j k l (left up down right)
nvim <filename> (open file with neovim)
i (insert mode)
I (go to start of line insert mode)
A (go to end of line insert mode)
a (go to rhs of current character insert mode)
O (go to line above insert mode)
o (go to line below insert mode)
:q (quits file)
:q! (force quit)
:wq (save and quit)
:w (save file)
:e (update open file to most recently saved version)
:<lineNumber> (takes you to line)
:PlugInstall (installs new plugin)
Ctrl-w <arrowKey> (switch panes)
V (select line)
v (select characters)
y (yank the selection)
p (paste on line below)
P (paste on line above)
d (cut selections)
D (delete rest of line)
C (clear rest of line and enter insert mode)
diw (delete word when you're inside it)
u (undo)
Ctrl-R (redo)
gg (go to top of file)
G (go to EOF)
0 (go to start of line)
$ (go to EOL)
w (go to beginning of next word)
e (go to last character of current word)
6w (skip to 6th word)
b (prev word)
( (end of sentence)
) (start of sentence)
{ (jump down code block)
} (jump up code block)
ctrl+d (half page down)
ctrl+u (half page up)
/ (search, n to go to next, N to go to prev)
* (go to next occurrence of what your cursor is on, then n and N to go back and forth)
f<character> (jump to on top of next occurrence of character, F to go backwards, ; to jump forward and , to jump backward through results)
t<character> (jump to just before next occurrence of character, T to go backwards, ; to jump forward and , to jump backward through results)
x (delete character)
s (delete character and enter insert mode)
S (delete entire line and enter insert mode)
cw (clear rest of word and enter insert mode)
dt<character> (delete up to just before character, e.g. dt) to delete everything inside () parentheses)
<num_lines><down_key> (go number of lines down)
<num_lines><up_key> (go number of lines up)
esc (go to command mode from insert mode)
:set paste (takes you to paste mode)
	p (paste)
	shift ins (paste from clipboard into terminal)
	:set nopaste (leave paste mode)
<map_leader>cs (sexy comment, any lang)
<map_leader>cu (uncomment, any lang)
<map_leader><c<space> (normal comment)
:%s/<find>/<replace>/gc (find and replace globally in file (%) with confirm message)
:.,+10s/<find>/<replace>/gc (find and replace from current line (.) to 10 lines down (+10) with confirm message)
> (indent current selection by 1 tab)
< (unindent current selection by 1 tab)
. (repeat previous command, e.g. to indent 3 lines by 2 tabs, would do Vjj>) (u to undo previous command)

# Splits (opening multiple 'buffers' in one tab)
<mapleader>s (split horizontally)
<mapeleader>v (split vertically)
ctrl+w o (close all buffers except current)
:resize 10 (set split to 10 rows tall)
:vertical resize 20 (set split to 20 columns wide)
ctrl+w = (equally size splits)
Tip: Rarely use these commands; prefer to quickly open and close any splits to focus on one buffer

# Navigation
:tab new (open new tab)
:E (list files & folders in dir)
<Enter> (select file/folder)
gt (go to right tab)
gT (go to left tab)
ctrl+o (walk backwards in navigation history)
ctrl+i (walk forwards in navigation history)

# Nerd tree navigation
ctrl+nt (open nerd tree)
t: (Open the selected file in a new tab)
i: (Open the selected file in a horizontal split window)
s: (Open the selected file in a vertical split window)
I: (Toggle hidden files)
m: (Show the NERD Tree menu)
R: (Refresh the tree, useful if files change outside of Vim)
?: (Toggle NERD Tree's quick help)
Tip: File tree can be helpful for ramping up on a new project but less helpful once you know where you want to go
Tip: ThePrimeagen uses splits not tabs, and tends not to have splits open for very long

# Fuzzy finder (fzf)
ctrl+p (open fuzzy finder)
ctrl+t (open result in new tab)
ctrl+^ (switch between last 2 files opened with fzf)
Tip: Have a main file that serves as hub, <Ctrl + p> to file you want to go to (spoke), and then <Ctrl + ^> back to hub (avoid hopping around in a triangle between more than 2 files)

# Format text (e.g. docstring) to 80 character width limit
1. Select line(s) to format with v
2. gq




------------------------------------------------------
# GIT
------------------------------------------------------
# basic
git add -u . (remove deleted files from github. Can then commit+push)
git remote -v (list remote origin)

# force pull
git fetch --all
git reset --hard origin/master
git pull origin master

# force push
git push <remote_url> --force

# change remote origin
git remote rm origin (remove origins)
git remote add origin <remote_url>
git config master.remote origin
git config master.merge refs/heads/master

# revert local repo to a previous commit (can then push)
git revert --no-commit <commit_unique_token>..HEAD
git commit

git branch # check branches (*<current_branch>)

# create new branch, make changes, merge changes back to master
git checkout -b <branch_name> (create new branch, git checkout <branch_name> to switch branches)
git add .
git commit -m "updated branch"
git push (push changes to branch)
git checkout master (switch to master branch)
git merge <branch_name> (merge branch_name with master)
git push (push merged changes to master branch)
git branch -d <branch_name> (delete branch you've now finished with)

# resolving merge conflicts
files with merge conflicts will have conflicts highlighted as:

<<<<<<< HEAD:index.html
<div id="footer">contact : email.support@github.com</div>
=======
<div id="footer">
 please contact us at support@github.com
</div>

git branch # check branches (*<current_branch>)
>>>>>>> iss53:index.html

where anything between <<<<<<<< HEAD and ======== is what was originally
in the file, and anything between ========= and >>>>>>>>>>> was what 
has since been added which is conflicting with the original. To resolve,
delete the highlight notation (i.e. delete <<<<<< HEAD, ====== etc) AND
either the original content or the new content so that only one is remaining,
then push.

# commited large file >100 MB GitHub limit
git fsck (find where dangling blob is)
git gc (do garbage collection) -> should now have <file_name> of file(s) taking up too much sapce in commit history
git filter-branch -f --tree-filter 'rm -f <file_name>' HEAD (remove <file_name> from all commit histories)
git push origin --force (push updated commit histories with large file(s) now removed)


------------------------------------------------------
# JEKYLL, BUNDLE & GITHUB PAGES
------------------------------------------------------
bundle exec jekyll serve --force_polling (build site locally, then http://localhost:4000) (N.B. If make change to .yml, must restart to refresh)






----------------------------------------------------
# SPHINX
------------------------------------------------------
# GETTING STARTED
# install the following packages with pip or conda
sphinx
nb2plots
sphinx_rtd_theme
# set up project with quickstart in root dir of proj
sphinx-quickstart
# move new files into e.g. docs/ dir at root of project, and in conf.py uncomment following to tell where proj root is
import os
import sys
sys.path.insert(0, os.path.abspath('.'))
# add following useful extensions to conf.py
extensions = [
        'sphinx_rtd_theme',
        'sphinx.ext.autodoc',
        'nb2plots',
        'sphinx.ext.napoleon',
        'sphinx.ext.viewcode',
        'texext'
]
# replace/add in conf.py:
html_theme = 'sphinx_rtd_theme'
pygments_style = 'friendly'
github_url = '<github_project_url>'
# set sphinx master doc for readthedocs to find in conf.py (otherwise get contents.rst not found)
master_doc = 'index'
# in <module_root>/docs, execute
make html

# build & overwrite docstring formatting
cd <module_root>/docs
sphinx-apidoc -f -o <dir with conf.py>/ <dir with package>/<package>
e.g. sphinx-apidoc -f -o docs/ ../../trafpy

# build auto-updating sphinx local website
cd <module_root>/docs
sphinx-autobuild docs docs/_build/html

# to host on readthedocs, need a .readthedocs.yml in project root dir. This will instruct readthedocs e.g. which packages to install when building documentation. See https://docs.readthedocs.io/en/stable/config-file/v2.html for example

# to generate pdf form, see: https://gist.github.com/alfredodeza/7fb5c667addb1c6963b9
# N.B. source -> directory where conf.py is, build/pdf -> where you want pdf to be generated
e.g. sphinx-build -b pdf docs/ docs/_build/pdf






----------------------------------------------------
# CONDA
------------------------------------------------------
# launch jupyter-notebook inside a conda environment
conda install ipykernel
python -m ipykernel install --user --name <env_name> --display-name "<env_display_name>" # make env appear as kernel in notebook
jupyter-notebook
# then click Kernel -> Change kernel -> <env_name>

# dead kernel
conda uninstall ipykernel
conda install ipykernel

# install all packages in requirements.txt with conda, if not available in conda install with pip
while read requirement; do conda install --yes $requirement || pip install $requirement; done < requirements.txt

# if installed package not found in notebook, check executable path in notebook and in terminal. If different, conda env executable path not set correctly
import sys
print(sys.executable)

# export environment to an environment.yml file
conda env export > environment.yml
# build an env from the environment.yml file (env name defined at top of .yml file)
conda env create -f environment.yml





----------------------------------------------------
# TENSORBOARD
------------------------------------------------------
# launch tensorboard on specific port
tensorboard --logdir <folder_containing_saved_log_files>/ --port=<tensorboard_port_number>



----------------------------------------------------
# PYTHON
------------------------------------------------------
# pause programme at certain line in code and then live edit code to debug
import pdb; pdb.set_trace()


# PYTORCH
# timing pytorch cuda functions with time profiling:
with torch.autograd.profiler.profile(use_cuda=True) as prof:
    ...
print(prof.table(sort_by='cuda_time_total'))






----------------------------------------------------
# SINGULARITY AND DOCKER
------------------------------------------------------
# in Dockerfile in <project_dir>, set WORKDIR path, then use docker build (building for first time takes a while)
docker build . -t <docker_project_name> -f Dockerfile
# once built, can enter docker project with docker run -> should then be in docker container and can run things
docker run -it --rm -p 8888:8888 --volume <project_dir>:<WORKDIR> <docker_project_name>

# get docker image id
docker images

# save compressed <file_name>.tar.gz docker image
docker save <docker_image_id> | gzip > <file_name>.tar.gz

# convert docker image to singularity image (need for e.g. remote servers which don't support docker) https://linuxize.com/post/how-to-extract-unzip-tar-gz-file/
# 1. save docker image on local or wherever docker container is already built (see above)
# 2. use scp to transfer saved container to remote (see above scp notes)
# 3. on remote, convert saved docker image to singularity image by building singularity image
/opt/singularity/bin/singularity build --sandbox <project_name> docker-archive://<file_name>.tar.gz
# run singularity container
/opt/singularity/bin/singularity run <project_name>






